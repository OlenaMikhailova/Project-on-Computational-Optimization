\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb} % For \mathbb{R}
\usepackage{amsfonts}

% --- Custom commands ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\|#1\|}          % Standard Euclidean norm
\newcommand{\normp}[1]{\|#1\|_P}       % P-norm
\newcommand{\ip}[2]{\langle #1, #2 \rangle} % Standard inner product
\newcommand{\grad}{\nabla}             % Standard gradient
\newcommand{\gradp}{\nabla_P}
\newcommand{\ipp}[2]{\langle #1, #2 \rangle_P} % P-inner product
% -----------------------

\begin{document}

\section*{Convergence of Standard Gradient Descent 1}

\subsection*{Problem Setup}
We consider the minimization problem:
\begin{equation*}
    \min_{x \in \R^n} f(x)
\end{equation*}
where $f: \R^n \to \R$ is a convex and differentiable function.
The standard gradient descent update rule is:
\begin{equation} \label{eq:gd_update}
    x_{k+1} = x_k - \alpha \grad f(x_k)
\end{equation}
where $\alpha > 0$ is the step size (learning rate), and $\grad f(x_k)$ is the gradient of $f$ at $x_k$ with respect to the standard Euclidean inner product $\ip{u}{v} = u^T v$. We use the standard Euclidean norm $\norm{u} = \sqrt{\ip{u}{u}}$.

\subsection*{Assumptions}
\begin{enumerate}
    \item \textbf{Convexity of $f$}: For any $x, y \in \R^n$:
          \begin{equation} \label{eq:convexity}
              f(y) \ge f(x) + \ip{\grad f(x)}{y-x}
          \end{equation}
    \item \textbf{$L$-smoothness (Lipschitz continuous gradient)}: The gradient $\grad f$ is $L$-Lipschitz continuous with respect to the Euclidean norm. That is, there exists a constant $L > 0$ such that:
          \begin{equation} \label{eq:l_smoothness}
              \norm{\grad f(x) - \grad f(y)} \le L \norm{x - y} \quad \forall x, y \in \R^n
          \end{equation}
          This assumption implies the following inequality (Descent Lemma):
          \begin{equation} \label{eq:descent_lemma}
              f(y) \le f(x) + \ip{\grad f(x)}{y-x} + \frac{L}{2} \norm{y-x}^2
          \end{equation}
    \item \textbf{Existence of a minimizer}: There exists $x^* \in \R^n$ such that $f(x^*) = f^* = \min_{x \in \R^n} f(x)$. For convex $f$, this implies $\grad f(x^*) = 0$.
\end{enumerate}

\subsection*{Convergence Proof}

\textbf{Step 1: Bounding the function decrease in one step.}
We use the Descent Lemma \eqref{eq:descent_lemma} with $x = x_k$ and $y = x_{k+1} = x_k - \alpha \grad f(x_k)$. Then $y - x = -\alpha \grad f(x_k)$.
\begin{align*}
    f(x_{k+1}) &\le f(x_k) + \ip{\grad f(x_k)}{-\alpha \grad f(x_k)} + \frac{L}{2} \norm{-\alpha \grad f(x_k)}^2 \\
    &= f(x_k) - \alpha \ip{\grad f(x_k)}{\grad f(x_k)} + \frac{L \alpha^2}{2} \norm{\grad f(x_k)}^2 \\
    &= f(x_k) - \alpha \norm{\grad f(x_k)}^2 + \frac{L \alpha^2}{2} \norm{\grad f(x_k)}^2 \\
    &= f(x_k) - \alpha \left( 1 - \frac{L \alpha}{2} \right) \norm{\grad f(x_k)}^2
\end{align*}
To guarantee descent, we choose the step size $\alpha$ such that $1 - \frac{L \alpha}{2} \ge 0$, i.e., $\alpha \le \frac{2}{L}$. A common choice is $\alpha = \frac{1}{L}$. With this choice:
\begin{align}
    f(x_{k+1}) &\le f(x_k) - \frac{1}{L} \left( 1 - \frac{L (1/L)}{2} \right) \norm{\grad f(x_k)}^2 \nonumber \\
    &= f(x_k) - \frac{1}{L} \left( 1 - \frac{1}{2} \right) \norm{\grad f(x_k)}^2 \nonumber \\
    f(x_{k+1}) &\le f(x_k) - \frac{1}{2 L} \norm{\grad f(x_k)}^2 \label{eq:func_decrease}
\end{align}
This shows that the function value decreases at each step, provided $\grad f(x_k) \neq 0$.

\textbf{Step 2: Analyzing the distance to the optimum.}
Consider the squared Euclidean distance from $x_{k+1}$ to $x^*$:
\begin{align*}
    \norm{x_{k+1} - x^*}^2 &= \norm{x_k - \alpha \grad f(x_k) - x^*}^2 \\
    &= \norm{(x_k - x^*) - \alpha \grad f(x_k)}^2 \\
    &= \norm{x_k - x^*}^2 - 2 \ip{x_k - x^*}{\alpha \grad f(x_k)} + \norm{\alpha \grad f(x_k)}^2 \\
    &= \norm{x_k - x^*}^2 - 2 \alpha \ip{\grad f(x_k)}{x_k - x^*} + \alpha^2 \norm{\grad f(x_k)}^2
\end{align*}

\textbf{Step 3: Using convexity.}
From the convexity inequality \eqref{eq:convexity}, substitute $y = x^*$:
\begin{equation*}
    f(x^*) \ge f(x_k) + \ip{\grad f(x_k)}{x^* - x_k}
\end{equation*}
Rearranging gives:
\begin{equation} \label{eq:convex_for_dist}
    \ip{\grad f(x_k)}{x_k - x^*} \ge f(x_k) - f(x^*) = f(x_k) - f^*
\end{equation}

\textbf{Step 4: Combining the results.}
Substitute inequality \eqref{eq:convex_for_dist} into the expression for the distance:
\begin{equation*}
    \norm{x_{k+1} - x^*}^2 \le \norm{x_k - x^*}^2 - 2 \alpha (f(x_k) - f^*) + \alpha^2 \norm{\grad f(x_k)}^2
\end{equation*}
Now, use the step size $\alpha = 1/L$ and the function decrease inequality \eqref{eq:func_decrease}. From \eqref{eq:func_decrease}, we have $\norm{\grad f(x_k)}^2 \le 2 L (f(x_k) - f(x_{k+1}))$.
\begin{align*}
    \norm{x_{k+1} - x^*}^2 &\le \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_k) - f^*) + \frac{1}{L^2} \norm{\grad f(x_k)}^2 \\
    &\le \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_k) - f^*) + \frac{1}{L^2} [2 L (f(x_k) - f(x_{k+1}))] \\
    &= \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_k) - f^*) + \frac{2}{L} (f(x_k) - f(x_{k+1})) \\
    &= \norm{x_k - x^*}^2 - \frac{2}{L} \left[ (f(x_k) - f^*) - (f(x_k) - f(x_{k+1})) \right] \\
    &= \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_{k+1}) - f^*)
\end{align*}

\textbf{Step 5: Telescoping sum.}
Let $\delta_k = \norm{x_k - x^*}^2$ (squared Euclidean distance) and $\varepsilon_k = f(x_k) - f^*$ (function error). The inequality becomes:
\begin{equation*}
    \delta_{k+1} \le \delta_k - \frac{2}{L} \varepsilon_{k+1}
\end{equation*}
or
\begin{equation*}
    \varepsilon_{k+1} \le \frac{L}{2} (\delta_k - \delta_{k+1})
\end{equation*}
Summing this inequality from $k = 0$ to $K-1$:
\begin{align*}
    \sum_{k=0}^{K-1} \varepsilon_{k+1} &\le \sum_{k=0}^{K-1} \frac{L}{2} (\delta_k - \delta_{k+1}) \\
    \sum_{k=1}^{K} \varepsilon_k &\le \frac{L}{2} \left( \sum_{k=0}^{K-1} (\delta_k - \delta_{k+1}) \right) \\
    &= \frac{L}{2} (\delta_0 - \delta_K) \quad \text{(Telescoping sum)}
\end{align*}
Since $\delta_K = \norm{x_K - x^*}^2 \ge 0$, we have $\delta_0 - \delta_K \le \delta_0 = \norm{x_0 - x^*}^2$. Thus:
\begin{equation} \label{eq:sum_epsilon}
    \sum_{k=1}^{K} \varepsilon_k \le \frac{L}{2} \norm{x_0 - x^*}^2
\end{equation}

\textbf{Step 6: Obtaining the convergence rate.}
From Step 1, we know that $f(x_{k+1}) \le f(x_k)$, so the sequence $\varepsilon_k = f(x_k) - f^*$ is non-increasing ($\varepsilon_{k+1} \le \varepsilon_k$). Therefore:
\begin{equation*}
    K \cdot \varepsilon_K = K (f(x_K) - f^*) \le \sum_{k=1}^{K} \varepsilon_k
\end{equation*}
Combining this with inequality \eqref{eq:sum_epsilon}:
\begin{equation*}
    K \varepsilon_K \le \frac{L}{2} \norm{x_0 - x^*}^2
\end{equation*}
From this, we obtain the convergence rate:
\begin{equation} \label{eq:convergence_rate}
    f(x_K) - f^* = \varepsilon_K \le \frac{L \norm{x_0 - x^*}^2}{2 K}
\end{equation}

\subsection*{Conclusion}
For a convex and $L$-smooth function $f$, the standard gradient descent method with step size $\alpha = 1/L$ converges in function value to the minimum $f^*$ with a rate of $O(1/K)$. That is, $f(x_K) \to f^*$ as $K \to \infty$.

\section*{Derivation of the Preconditioned Gradient $\gradp f(x)$}

Let $f: \R^n \to \R$ be a differentiable function. Let $P$ be a symmetric positive definite matrix defining the $P$-inner product :
\begin{align*}
    \ipp{u}{v} &= u^T P v = \ip{Pu}{v} \\
    \normp{u} &= \sqrt{\ipp{u}{u}} = \sqrt{u^T P u}
\end{align*}
The differential of $f$ at $x$, denoted $df_x(h)$, is a linear functional representing the best linear approximation of the change $f(x+h) - f(x)$ for a small displacement $h$.
By the Riesz Representation Theorem, this linear functional $df_x$ can be represented via an inner product with a unique vector. The specific vector depends on the chosen inner product:

\begin{enumerate}
    \item Using the \textbf{standard inner product} $\ip{\cdot}{\cdot}$, there exists a unique vector, the \textbf{standard gradient} $\grad f(x)$, such that for all $h \in \R^n$:
        \begin{equation*}
            df_x(h) = \ip{\grad f(x)}{h}
        \end{equation*}
    \item Using the \textbf{$P$-inner product} $\ipp{\cdot}{\cdot}$, there exists a unique vector, the \textbf{preconditioned gradient} $\gradp f(x)$, such that for all $h \in \R^n$:
        \begin{equation*}
            df_x(h) = \ipp{\gradp f(x)}{h}
        \end{equation*}
\end{enumerate}

Since both expressions represent the same differential $df_x(h)$, they must be equal:
\begin{equation*}
    \ip{\grad f(x)}{h} = \ipp{\gradp f(x)}{h}
\end{equation*}
Using the definition $\ipp{u}{v} = \ip{Pu}{v}$:
\begin{equation*}
    \ip{\grad f(x)}{h} = \ip{P (\gradp f(x))}{h}
\end{equation*}
Rearranging the terms:
\begin{equation*}
    \ip{\grad f(x) - P \gradp f(x)}{h} = 0
\end{equation*}
This must hold for all $h$, which implies the vector inside the inner product must be zero:
\begin{equation*}
    \grad f(x) - P \gradp f(x) = 0
\end{equation*}
Solving for $\gradp f(x)$ (using the invertibility of $P$):
\begin{equation*}
    P \gradp f(x) = \grad f(x)
\end{equation*}
\begin{equation} \label{eq:gradp_explicit_app}
    \boxed{\gradp f(x) = P^{-1} \grad f(x)}
\end{equation}
This gives the explicit relationship between the preconditioned gradient and the standard gradient.

\vspace{\baselineskip}

\section*{Convergence of Preconditioned Gradient Descent 1}

\subsection*{Problem Setup}
We consider the minimization problem:
\begin{equation*}
    \min_{x \in \R^n} f(x)
\end{equation*}
where $f: \R^n \to \R$ is a convex and differentiable function.
The preconditioned gradient descent update rule is:
\begin{equation} \label{eq:pgd_update}
    x_{k+1} = x_k - \alpha \gradp f(x_k) = x_k - \alpha P^{-1} \grad f(x_k)
\end{equation}
where $\alpha > 0$ is the step size (learning rate).

\subsection*{Assumptions}
\begin{enumerate}
    \item \textbf{Convexity of $f$}: For any $x, y \in \R^n$:
          \begin{equation} \label{eq:convexity_p}
              f(y) \ge f(x) + \ip{\grad f(x)}{y-x} = f(x) + \ipp{\gradp f(x)}{y-x}
          \end{equation}
    \item \textbf{$L_P$-smoothness with respect to the $P$-norm}: We assume that the preconditioned gradient $\gradp f$ is $L_P$-Lipschitz continuous with respect to the $P$-norm. That is, there exists a constant $L_P > 0$ such that:
          \begin{equation} \label{eq:lp_smoothness}
              \normp{\gradp f(x) - \gradp f(y)} \le L_P \normp{x - y} \quad \forall x, y \in \R^n
          \end{equation}
          This assumption is equivalent to the following inequality (the $P$-Descent Lemma):
          \begin{equation} \label{eq:p_descent_lemma}
              f(y) \le f(x) + \ipp{\gradp f(x)}{y-x} + \frac{L_P}{2} \normp{y-x}^2
          \end{equation}
          
    \item \textbf{Existence of a minimizer}: There exists $x^* \in \R^n$ such that $f(x^*) = f^* = \min_{x \in \R^n} f(x)$. For convex $f$, this implies $\grad f(x^*) = 0$, and consequently $\gradp f(x^*) = P^{-1} 0 = 0$.
\end{enumerate}

\subsection*{Convergence Proof}

\textbf{Step 1: Bounding the function decrease in one step.}
We use the $P$-Descent Lemma \eqref{eq:p_descent_lemma} with $x = x_k$ and $y = x_{k+1} = x_k - \alpha \gradp f(x_k)$. Then $y - x = -\alpha \gradp f(x_k)$.
\begin{align*}
    f(x_{k+1}) &\le f(x_k) + \ipp{\gradp f(x_k)}{-\alpha \gradp f(x_k)} + \frac{L_P}{2} \normp{-\alpha \gradp f(x_k)}^2 \\
    &= f(x_k) - \alpha \ipp{\gradp f(x_k)}{\gradp f(x_k)} + \frac{L_P \alpha^2}{2} \normp{\gradp f(x_k)}^2 \\
    &= f(x_k) - \alpha \normp{\gradp f(x_k)}^2 + \frac{L_P \alpha^2}{2} \normp{\gradp f(x_k)}^2 \\
    &= f(x_k) - \alpha \left( 1 - \frac{L_P \alpha}{2} \right) \normp{\gradp f(x_k)}^2
\end{align*}
To guarantee descent, we choose the step size $\alpha$ such that $1 - \frac{L_P \alpha}{2} \ge 0$, i.e., $\alpha \le \frac{2}{L_P}$. A standard choice is $\alpha = \frac{1}{L_P}$. With this choice:
\begin{align}
    f(x_{k+1}) &\le f(x_k) - \frac{1}{L_P} \left( 1 - \frac{L_P (1/L_P)}{2} \right) \normp{\gradp f(x_k)}^2 \nonumber \\
    &= f(x_k) - \frac{1}{L_P} \left( 1 - \frac{1}{2} \right) \normp{\gradp f(x_k)}^2 \nonumber \\
    f(x_{k+1}) &\le f(x_k) - \frac{1}{2 L_P} \normp{\gradp f(x_k)}^2 \label{eq:func_decrease_p}
\end{align}
This shows that the function value decreases at each step, provided $\gradp f(x_k) \neq 0$.

\textbf{Step 2: Analyzing the distance to the optimum in $P$-norm.}
Consider the squared $P$-norm of the distance from $x_{k+1}$ to $x^*$:
\begin{align*}
    \normp{x_{k+1} - x^*}^2 &= \normp{x_k - \alpha \gradp f(x_k) - x^*}^2 \\
    &= \normp{(x_k - x^*) - \alpha \gradp f(x_k)}_P^2 \\
    &= \normp{x_k - x^*}^2 - 2 \ipp{x_k - x^*}{\alpha \gradp f(x_k)} + \normp{\alpha \gradp f(x_k)}^2 \\
    &= \normp{x_k - x^*}^2 - 2 \alpha \ipp{\gradp f(x_k)}{x_k - x^*} + \alpha^2 \normp{\gradp f(x_k)}^2
\end{align*}

\textbf{Step 3: Using convexity.}
From the convexity inequality \eqref{eq:convexity_p}, substitute $y = x^*$:
\begin{equation*}
    f(x^*) \ge f(x_k) + \ipp{\gradp f(x_k)}{x^* - x_k}
\end{equation*}
Rearranging gives:
\begin{equation} \label{eq:convex_for_dist_p}
    \ipp{\gradp f(x_k)}{x_k - x^*} \ge f(x_k) - f(x^*) = f(x_k) - f^*
\end{equation}


\textbf{Step 4: Combining the results.}
Substitute inequality \eqref{eq:convex_for_dist_p} into the expression for the distance:
\begin{equation*}
    \normp{x_{k+1} - x^*}^2 \le \normp{x_k - x^*}^2 - 2 \alpha (f(x_k) - f^*) + \alpha^2 \normp{\gradp f(x_k)}^2
\end{equation*}
Now, use the step size $\alpha = 1/L_P$ and the function decrease inequality \eqref{eq:func_decrease_p}. From \eqref{eq:func_decrease_p}, we have $\normp{\gradp f(x_k)}^2 \le 2 L_P (f(x_k) - f(x_{k+1}))$.
\begin{align*}
    \normp{x_{k+1} - x^*}^2 &\le \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_k) - f^*) + \frac{1}{L_P^2} \normp{\gradp f(x_k)}^2 \\
    &\le \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_k) - f^*) + \frac{1}{L_P^2} [2 L_P (f(x_k) - f(x_{k+1}))] \\
    &= \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_k) - f^*) + \frac{2}{L_P} (f(x_k) - f(x_{k+1})) \\
    &= \normp{x_k - x^*}^2 - \frac{2}{L_P} \left[ (f(x_k) - f^*) - (f(x_k) - f(x_{k+1})) \right] \\
    &= \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_{k+1}) - f^*)
\end{align*}
\textbf{Step 5: Telescoping sum.}
Let $\delta_k^P = \normp{x_k - x^*}^2$ (squared $P$-distance) and $\varepsilon_k = f(x_k) - f^*$ (function error). The inequality becomes:
\begin{equation*}
    \delta_{k+1}^P \le \delta_k^P - \frac{2}{L_P} \varepsilon_{k+1}
\end{equation*}
or
\begin{equation*}
    \varepsilon_{k+1} \le \frac{L_P}{2} (\delta_k^P - \delta_{k+1}^P)
\end{equation*}
Summing this inequality from $k = 0$ to $K-1$:
\begin{align*}
    \sum_{k=0}^{K-1} \varepsilon_{k+1} &\le \sum_{k=0}^{K-1} \frac{L_P}{2} (\delta_k^P - \delta_{k+1}^P) \\
    \sum_{k=1}^{K} \varepsilon_k &\le \frac{L_P}{2} \left( \sum_{k=0}^{K-1} (\delta_k^P - \delta_{k+1}^P) \right) \\
    &= \frac{L_P}{2} (\delta_0^P - \delta_K^P) \quad \text{(Telescoping sum)}
\end{align*}
Since $\delta_K^P = \normp{x_K - x^*}^2 \ge 0$, we have $\delta_0^P - \delta_K^P \le \delta_0^P = \normp{x_0 - x^*}^2$. Thus:
\begin{equation} \label{eq:sum_epsilon_p}
    \sum_{k=1}^{K} \varepsilon_k \le \frac{L_P}{2} \normp{x_0 - x^*}^2
\end{equation}

\textbf{Step 6: Obtaining the convergence rate.}
From Step 1, we know that $f(x_{k+1}) \le f(x_k)$, so the sequence $\varepsilon_k = f(x_k) - f^*$ is non-increasing ($\varepsilon_{k+1} \le \varepsilon_k$). Therefore:
\begin{equation*}
    K \cdot \varepsilon_K = K (f(x_K) - f^*) \le \sum_{k=1}^{K} \varepsilon_k
\end{equation*}
Combining this with inequality \eqref{eq:sum_epsilon_p}:
\begin{equation*}
    K \varepsilon_K \le \frac{L_P}{2} \normp{x_0 - x^*}^2
\end{equation*}
From this, we obtain the convergence rate:
\begin{equation} \label{eq:convergence_rate_p}
    f(x_K) - f^* = \varepsilon_K \le \frac{L_P \normp{x_0 - x^*}^2}{2 K}
\end{equation}

\subsection*{Conclusion}
For a convex function $f$ that is $L_P$-smooth with respect to the $P$-norm, the preconditioned gradient descent method with step size $\alpha = 1/L_P$ converges in function value to the minimum $f^*$ with a rate of $O(1/K)$. That is, $f(x_K) \to f^*$ as $K \to \infty$.

The proof is completely analogous to the standard proof for GD, but all operations (inner product, norm, gradient, Lipschitz constant) are replaced by their $P$-analogues.

\section*{Base Statements (for second part)}

\paragraph{Lemma}
If $f$ is $L$–smooth and $\gamma > 0$, then for all $x, y \in \mathbb{R}^d$,
\[
f(x - \gamma \nabla f(x)) - f(x) \le -\gamma \left(1 - \frac{\gamma L}{2} \right) \|\nabla f(x)\|^2. \tag{10}
\]
If moreover $\inf f > -\infty$, then for all $x \in \mathbb{R}^d$,
\[
\frac{1}{2L} \|\nabla f(x)\|^2 \le f(x) - \inf f.
\]



\section*{General Proof of Convergence of Gradient Descent 2}

\textbf{Theorem} Consider the Problem (Differentiable Function) and assume that \( f \) is convex
and \( L \)-smooth, for some \( L > 0 \). Let \( (x_t)_{t\in\mathbb{N}} \) be the sequence of iterates generated by the (GD)
algorithm, with a stepsize satisfying \( 0 < \gamma \le \frac{1}{L} \). Then, for all \( x^* \in \arg\min f \), for all \( t \in \mathbb{N} \), we have:
\[
f(x_t) - \inf f \le \frac{\|x_0 - x^*\|^2}{2\gamma t}.
\]

\textbf{Proof} Let $f$ be convex and $L$–smooth. It follows that
\begin{align*}
\|x_{t+1} - x^*\|^2 
&= \left\|x_t - x^* - \frac{1}{L} \nabla f(x_t) \right\|^2 \\
&= \|x_t - x^*\|^2 
   - 2 \cdot \frac{1}{L} \langle x_t - x^*, \nabla f(x_t) \rangle 
   + \frac{1}{L^2} \|\nabla f(x_t)\|^2  \\
&\overset{(1)}{\le} \|x_t - x^*\|^2 - \frac{1}{L^2} \|\nabla f(x_t)\|^2. \tag{18}
\end{align*}

Thus, \( \|x_t - x^*\|^2 \) is a decreasing sequence in \( t \), and consequently
\begin{equation}
\|x_t - x^*\| \le \|x_0 - x^*\|. \tag{19}
\end{equation}

Calling upon (10) and subtracting \( f(x^*) \) from both sides gives
\begin{equation}
f(x_{t+1}) - f(x^*) \le f(x_t) - f(x^*) - \frac{1}{2L} \|\nabla f(x_t)\|^2. \tag{20}
\end{equation}

Applying convexity we have that
\begin{align}
f(x_t) - f(x^*) &\le \langle \nabla f(x_t), x_t - x^* \rangle \nonumber \\
&\le \|\nabla f(x_t)\| \cdot \|x_t - x^*\| \nonumber \\
&\overset{(19)}{\le} \|\nabla f(x_t)\| \cdot \|x_0 - x^*\|. \tag{21}
\end{align}

Suppose now that \( x_0 \neq x^* \), otherwise the proof is finished. Isolating \( \| \nabla f(x_t) \| \) in the above and inserting in (20) gives
\[
f(x_{t+1}) - f(x^*) \overset{(20) + (21)}{\leq} f(x_t) - f(x^*) - \frac{1}{2L} \frac{1}{\|x_0 - x^*\|^2} ( f(x_t) - f(x^*) )^2  \tag{22}
\]

Let \( \beta = \frac{1}{2L} \frac{1}{\|x_0 - x^*\|^2} \)  \quad  and \( \delta_t = f(x_t) - f(x^*) \). Since \( \delta_{t+1} \leq \delta_t \), and by manipulating (22) we have that

\[
\delta_{t+1} \leq \delta_t - \beta \delta_t^2 \xleftrightarrow{\times \frac{1}{\delta_t \delta_{t+1}}}
\beta \frac{\delta_t}{\delta_{t+1}} \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t}
\xleftrightarrow{\delta_{t+1} \leq \delta_t}
\beta \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t}.
\]

Summing up both sides over $t = 0, \dots, T - 1$ and using telescopic cancellation we have that
\[
T \beta \leq \frac{1}{\delta_T} - \frac{1}{\delta_0} \leq \frac{1}{\delta_T}.
\]

Re-arranging the above we have that
\[
f(x^T) - f(x^*) = \delta_T \leq \frac{1}{\beta T} = \frac{2L \|x^0 - x^*\|^2}{T}.
\]

%\section*{Proof of Convergence of Gradient Descent with weighted inner product}

%\textbf{Proof.} Consider the norm induced by \(P\): \( \|x\|_P^2 = x^\top P x \). Then,
%\begin{align*}
%\|x_{t+1} - x^*\|_P^2 
%&= \left\|x_t - x^* - \eta P^{-1} \nabla f(x_t)\right\|_P^2 \\
%&= \|x_t - x^*\|_P^2 - 2\eta \langle \nabla f(x_t), x_t - x^* \rangle + \eta^2 \|\nabla f(x_t)\|_{P^{-1}}^2,
%\end{align*}
%where \( \|\nabla f(x_t)\|_{P^{-1}}^2 = \nabla f(x_t)^\top P^{-1} \nabla f(x_t) \).

%From this, we obtain:
%\[
%\|x_{t+1} - x^*\|_P^2 \le \|x_t - x^*\|_P^2 - 2\eta \langle \nabla f(x_t), x_t - x^* \rangle + \eta^2 \|\nabla f(x_t)\|_{P^{-1}}^2.
%\]

%Since \(f\) is convex and \(L_P\)-smooth in the \(P\)-norm, we can use the inequality:
%\[
%f(x_{t+1}) \le f(x_t) - \eta \|\nabla f(x_t)\|_{P^{-1}}^2 + \frac{L_P \eta^2}{2} \|\nabla f(x_t)\|_{P^{-1}}^2
%= f(x_t) - \left(\eta - \frac{L_P \eta^2}{2} \right)\|\nabla f(x_t)\|_{P^{-1}}^2.
%\]

%Choosing \( \eta = \frac{1}{L_P} \), we simplify the inequality:
%\[
%f(x_{t+1}) \le f(x_t) - \frac{1}{2 L_P} \|\nabla f(x_t)\|_{P^{-1}}^2.
%\]

%Now using convexity again:
%\[
%f(x_t) - f(x^*) \le \langle \nabla f(x_t), x_t - x^* \rangle \le \|\nabla f(x_t)\|_{P^{-1}} \cdot \|x_t - x^*\|_P \le \|\nabla f(x_t)\|_{P^{-1}} \cdot \|x_0 - x^*\|_P.
%\]

%Solving for \( \|\nabla f(x_t)\|_{P^{-1}} \) and substituting gives:
%\[
%f(x_{t+1}) - f(x^*) \le f(x_t) - f(x^*) - \frac{1}{2 L_P} \cdot \frac{(f(x_t) - f(x^*))^2}{\|x_0 - x^*\|_P^2}.
%\]

%Letting \( \delta_t := f(x_t) - f(x^*) \), and \( \beta = \frac{1}{2 L_P \|x_0 - x^*\|_P^2} \), we get:
%\[
%\delta_{t+1} \le \delta_t - \beta \delta_t^2.
%\]

%Then, as before:
%\[
%\beta \le \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t} \quad \Rightarrow \quad T \beta \le \frac{1}{\delta_T} - \frac{1}{\delta_0} \le \frac{1}{\delta_T},
%\]
%hence:
%\[
%f(x_T) - f(x^*) = \delta_T \le \frac{1}{\beta T} = \frac{2 L_P \|x^0 - x^*\|_P^2}{T}.
%\]
%\qed

\section*{Proof of Convergence of Gradient Descent with weighted inner product 2}

From now on, we will use the following notions:

\begin{align*}
    \nabla f(x) &= P \nabla_P f(x), \\
    P^{-1} \nabla f(x) &= \nabla_P f(x), \\
    x_{t+1} &= x_t - \eta \nabla_P f(x_t) 
    \quad \Leftrightarrow \quad 
    x_{t+1} = x_t - \eta P^{-1} \nabla f(x_t).
\end{align*}

If you see an inner product written without the subscript \(P\), this is done deliberately and refers to the standard Euclidean inner product. 

\textbf{Proof.} Consider the norm induced by \(P\): \( \|x\|_P^2 = x^\top P x \). Then the gradient step becomes
\[
x_{t+1} = x_t - \eta P^{-1} \nabla f(x_t),
\]
which can be written as
\[
x_{t+1} - x^* = x_t - x^* - \eta P^{-1} \nabla f(x_t).
\]
Taking the squared \(P\)-norm of both sides:
\begin{align*}
\|x_{t+1} - x^*\|_P^2 
&= \left\|x_t - x^* - \eta P^{-1} \nabla f(x_t)\right\|_P^2 \\
&= \|x_t - x^*\|_P^2 - 2\eta \langle P^{-1} \nabla f(x_t), x_t - x^* \rangle_P + \eta^2 \|P^{-1} \nabla f(x_t)\|_P^2 \\
&= \|x_t - x^*\|_P^2 - 2\eta \langle \nabla f(x_t), x_t - x^* \rangle + \eta^2 \nabla f(x_t)^\top P^{-1} \nabla f(x_t),
\end{align*}
where we used the identity \(\langle u, v \rangle_P = u^\top P v\) and the fact that \(P P^{-1} = I\).

Now, suppose \(f\) is convex and \(L_P\)-smooth with respect to the \(P\)-norm. Then, from standard smoothness inequality:
\[
f(x_{t+1}) \le f(x_t) + \langle \nabla f(x_t), x_{t+1} - x_t \rangle + \frac{L_P}{2} \|x_{t+1} - x_t\|_P^2. \tag{?}
\]
Substitute \(x_{t+1} - x_t = -\eta P^{-1} \nabla f(x_t)\):
\begin{align*}
f(x_{t+1}) 
&\le f(x_t) - \eta \nabla f(x_t)^\top P^{-1} \nabla f(x_t) + \frac{L_P \eta^2}{2} \nabla f(x_t)^\top P^{-1} \nabla f(x_t) \\
&= f(x_t) - \left( \eta - \frac{L_P \eta^2}{2} \right) \nabla f(x_t)^\top P^{-1} \nabla f(x_t).
\end{align*}

Choosing \( \eta = \frac{1}{L_P} \), we obtain:
\[
f(x_{t+1}) \le f(x_t) - \frac{1}{2 L_P} \nabla f(x_t)^\top P^{-1} \nabla f(x_t).
\]

From convexity, we also have:
\[
f(x_t) - f(x^*) \le \langle \nabla f(x_t), x_t - x^* \rangle.
\]
Using Cauchy-Schwarz in \(P\)-norm (It was assumed that Cauchy-Schwarz inequality hold in any weighted inner product space? \href{https://math.stackexchange.com/questions/463073/why-does-the-cauchy-schwarz-inequality-hold-in-any-inner-product-space}{Link}):
\[
\langle \nabla f(x_t), x_t - x^* \rangle \le \|x_t - x^*\|_P \cdot \|P^{-1} \nabla f(x_t)\|_P.
\]
Note that:
\[
\|P^{-1} \nabla f(x_t)\|_P^2 = \nabla f(x_t)^\top P^{-1} \nabla f(x_t).
\]

So we get:
\[
f(x_t) - f(x^*) \le \|x_t - x^*\|_P \cdot \sqrt{\nabla f(x_t)^\top P^{-1} \nabla f(x_t)} \le \|x_0 - x^*\|_P \cdot \sqrt{\nabla f(x_t)^\top P^{-1} \nabla f(x_t)}.
\]

Solving for \( \nabla f(x_t)^\top P^{-1} \nabla f(x_t) \) and plugging into the earlier bound:
\[
f(x_{t+1}) - f(x^*) \le f(x_t) - f(x^*) - \frac{1}{2 L_P} \cdot \frac{(f(x_t) - f(x^*))^2}{\|x_0 - x^*\|_P^2}.
\]

Letting \( \delta_t = f(x_t) - f(x^*) \), and \( \beta = \frac{1}{2 L_P \|x_0 - x^*\|_P^2} \), we obtain:
\[
\delta_{t+1} \le \delta_t - \beta \delta_t^2.
\]

As in standard analysis, we get ($t = 0, \dots, T - 1$):

\[
\delta_{t+1} \leq \delta_t - \beta \delta_t^2 \xleftrightarrow{\times \frac{1}{\delta_t \delta_{t+1}}}
\beta \frac{\delta_t}{\delta_{t+1}} \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t}
\xleftrightarrow{\delta_{t+1} \leq \delta_t}
\beta \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t}.
\]

\[
\beta \le \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t} \quad \Rightarrow \quad T \beta \le \frac{1}{\delta_T} - \frac{1}{\delta_0} \le \frac{1}{\delta_T},
\]
which implies:
\[
f(x_T) - f(x^*) = \delta_T \le \frac{1}{\beta T} = \frac{2 L_P \|x^0 - x^*\|_P^2}{T}.
\]
\qed


\end{document}

\end{document}