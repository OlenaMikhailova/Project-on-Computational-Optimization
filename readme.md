# Project


# Tasks

1.  Learn theory for GD (convex case vs nonconvex).

    **Keywords:** convexity, strong convexity, descent lemma, stepsize,
    gradient descent, linesearch, metric projection, convergence rate.
    References are given below. If you want, you can (together) make
    your notes of what you have learned, but I leave it up to you.

2.  You should be able to see your partner’s email. Please write to each
    other and decide how you prefer to communicate.

3.  Create a repository together on [Github](https://github.com). Add me
    with the `y.malitsky@gmail.com` and Rostyslav with
    `rhryniv@ucu.edu.ua` The repository should contain the tex file with
    the project (empty for now or with your notes), the folder with code
    (for future) and this `readme` file (if you want).

4.  Let me know when you are ready. For the meeting, I expect you to know all
    the terminology and understand basic proofs. For now, I think two weeks is
    enough, but let me know if you want to meet sooner or if you need more
    time. I will test you during our next meeting, so don’t take it too lightly.

# Reading

You don’t have to read all of this. Just find the style you like best
and read a few references so that at least all the keywords are covered.

1.  <https://www.mit.edu/~gfarina/2024/67220s24_L07_gradient_descent/L07.pdf>
2.  <https://www.stat.cmu.edu/~ryantibs/convexopt/>
3.  <https://www.stat.cmu.edu/~siva/teaching/725/>
4.  [My two
    lectures](https://www.dropbox.com/scl/fi/73h3g7o6auisotnzvz9cp/lect_1-2.pdf?rlkey=vpufgiys4wuyhrmie33xiks40&st=p9emn9g7&dl=0),
    but they are very light, almost without proofs.
5.  Yin Tat Lee, Santosh Vempala [“Continuous Algorithms: Optimization
    and
    Sampling”](https://github.com/YinTat/optimizationbook/blob/main/main.pdfw)
    Pages 1–14 and 19–28.
