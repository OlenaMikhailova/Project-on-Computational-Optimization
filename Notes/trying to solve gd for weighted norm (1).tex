\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb} % For \mathbb{R}
\usepackage{amsfonts}

% --- Custom commands ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\|#1\|}          % Standard Euclidean norm
\newcommand{\normp}[1]{\|#1\|_P}       % P-norm
\newcommand{\ip}[2]{\langle #1, #2 \rangle} % Standard inner product
\newcommand{\grad}{\nabla}             % Standard gradient
\newcommand{\gradp}{\nabla_P}
\newcommand{\ipp}[2]{\langle #1, #2 \rangle_P} % P-inner product
% -----------------------

\begin{document}

\section*{Convergence of Standard Gradient Descent}

\subsection*{Problem Setup}
We consider the minimization problem:
\begin{equation*}
    \min_{x \in \R^n} f(x)
\end{equation*}
where $f: \R^n \to \R$ is a convex and differentiable function.
The standard gradient descent update rule is:
\begin{equation} \label{eq:gd_update}
    x_{k+1} = x_k - \alpha \grad f(x_k)
\end{equation}
where $\alpha > 0$ is the step size (learning rate), and $\grad f(x_k)$ is the gradient of $f$ at $x_k$ with respect to the standard Euclidean inner product $\ip{u}{v} = u^T v$. We use the standard Euclidean norm $\norm{u} = \sqrt{\ip{u}{u}}$.

\subsection*{Assumptions}
\begin{enumerate}
    \item \textbf{Convexity of $f$}: For any $x, y \in \R^n$:
          \begin{equation} \label{eq:convexity}
              f(y) \ge f(x) + \ip{\grad f(x)}{y-x}
          \end{equation}
    \item \textbf{$L$-smoothness (Lipschitz continuous gradient)}: The gradient $\grad f$ is $L$-Lipschitz continuous with respect to the Euclidean norm. That is, there exists a constant $L > 0$ such that:
          \begin{equation} \label{eq:l_smoothness}
              \norm{\grad f(x) - \grad f(y)} \le L \norm{x - y} \quad \forall x, y \in \R^n
          \end{equation}
          This assumption implies the following inequality (Descent Lemma):
          \begin{equation} \label{eq:descent_lemma}
              f(y) \le f(x) + \ip{\grad f(x)}{y-x} + \frac{L}{2} \norm{y-x}^2
          \end{equation}
    \item \textbf{Existence of a minimizer}: There exists $x^* \in \R^n$ such that $f(x^*) = f^* = \min_{x \in \R^n} f(x)$. For convex $f$, this implies $\grad f(x^*) = 0$.
\end{enumerate}

\subsection*{Convergence Proof}

\textbf{Step 1: Bounding the function decrease in one step.}
We use the Descent Lemma \eqref{eq:descent_lemma} with $x = x_k$ and $y = x_{k+1} = x_k - \alpha \grad f(x_k)$. Then $y - x = -\alpha \grad f(x_k)$.
\begin{align*}
    f(x_{k+1}) &\le f(x_k) + \ip{\grad f(x_k)}{-\alpha \grad f(x_k)} + \frac{L}{2} \norm{-\alpha \grad f(x_k)}^2 \\
    &= f(x_k) - \alpha \ip{\grad f(x_k)}{\grad f(x_k)} + \frac{L \alpha^2}{2} \norm{\grad f(x_k)}^2 \\
    &= f(x_k) - \alpha \norm{\grad f(x_k)}^2 + \frac{L \alpha^2}{2} \norm{\grad f(x_k)}^2 \\
    &= f(x_k) - \alpha \left( 1 - \frac{L \alpha}{2} \right) \norm{\grad f(x_k)}^2
\end{align*}
To guarantee descent, we choose the step size $\alpha$ such that $1 - \frac{L \alpha}{2} \ge 0$, i.e., $\alpha \le \frac{2}{L}$. A common choice is $\alpha = \frac{1}{L}$. With this choice:
\begin{align}
    f(x_{k+1}) &\le f(x_k) - \frac{1}{L} \left( 1 - \frac{L (1/L)}{2} \right) \norm{\grad f(x_k)}^2 \nonumber \\
    &= f(x_k) - \frac{1}{L} \left( 1 - \frac{1}{2} \right) \norm{\grad f(x_k)}^2 \nonumber \\
    f(x_{k+1}) &\le f(x_k) - \frac{1}{2 L} \norm{\grad f(x_k)}^2 \label{eq:func_decrease}
\end{align}
This shows that the function value decreases at each step, provided $\grad f(x_k) \neq 0$.

\textbf{Step 2: Analyzing the distance to the optimum.}
Consider the squared Euclidean distance from $x_{k+1}$ to $x^*$:
\begin{align*}
    \norm{x_{k+1} - x^*}^2 &= \norm{x_k - \alpha \grad f(x_k) - x^*}^2 \\
    &= \norm{(x_k - x^*) - \alpha \grad f(x_k)}^2 \\
    &= \norm{x_k - x^*}^2 - 2 \ip{x_k - x^*}{\alpha \grad f(x_k)} + \norm{\alpha \grad f(x_k)}^2 \\
    &= \norm{x_k - x^*}^2 - 2 \alpha \ip{\grad f(x_k)}{x_k - x^*} + \alpha^2 \norm{\grad f(x_k)}^2
\end{align*}

\textbf{Step 3: Using convexity.}
From the convexity inequality \eqref{eq:convexity}, substitute $y = x^*$:
\begin{equation*}
    f(x^*) \ge f(x_k) + \ip{\grad f(x_k)}{x^* - x_k}
\end{equation*}
Rearranging gives:
\begin{equation} \label{eq:convex_for_dist}
    \ip{\grad f(x_k)}{x_k - x^*} \ge f(x_k) - f(x^*) = f(x_k) - f^*
\end{equation}

\textbf{Step 4: Combining the results.}
Substitute inequality \eqref{eq:convex_for_dist} into the expression for the distance:
\begin{equation*}
    \norm{x_{k+1} - x^*}^2 \le \norm{x_k - x^*}^2 - 2 \alpha (f(x_k) - f^*) + \alpha^2 \norm{\grad f(x_k)}^2
\end{equation*}
Now, use the step size $\alpha = 1/L$ and the function decrease inequality \eqref{eq:func_decrease}. From \eqref{eq:func_decrease}, we have $\norm{\grad f(x_k)}^2 \le 2 L (f(x_k) - f(x_{k+1}))$.
\begin{align*}
    \norm{x_{k+1} - x^*}^2 &\le \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_k) - f^*) + \frac{1}{L^2} \norm{\grad f(x_k)}^2 \\
    &\le \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_k) - f^*) + \frac{1}{L^2} [2 L (f(x_k) - f(x_{k+1}))] \\
    &= \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_k) - f^*) + \frac{2}{L} (f(x_k) - f(x_{k+1})) \\
    &= \norm{x_k - x^*}^2 - \frac{2}{L} \left[ (f(x_k) - f^*) - (f(x_k) - f(x_{k+1})) \right] \\
    &= \norm{x_k - x^*}^2 - \frac{2}{L} (f(x_{k+1}) - f^*)
\end{align*}

\textbf{Step 5: Telescoping sum.}
Let $\delta_k = \norm{x_k - x^*}^2$ (squared Euclidean distance) and $\varepsilon_k = f(x_k) - f^*$ (function error). The inequality becomes:
\begin{equation*}
    \delta_{k+1} \le \delta_k - \frac{2}{L} \varepsilon_{k+1}
\end{equation*}
or
\begin{equation*}
    \varepsilon_{k+1} \le \frac{L}{2} (\delta_k - \delta_{k+1})
\end{equation*}
Summing this inequality from $k = 0$ to $K-1$:
\begin{align*}
    \sum_{k=0}^{K-1} \varepsilon_{k+1} &\le \sum_{k=0}^{K-1} \frac{L}{2} (\delta_k - \delta_{k+1}) \\
    \sum_{k=1}^{K} \varepsilon_k &\le \frac{L}{2} \left( \sum_{k=0}^{K-1} (\delta_k - \delta_{k+1}) \right) \\
    &= \frac{L}{2} (\delta_0 - \delta_K) \quad \text{(Telescoping sum)}
\end{align*}
Since $\delta_K = \norm{x_K - x^*}^2 \ge 0$, we have $\delta_0 - \delta_K \le \delta_0 = \norm{x_0 - x^*}^2$. Thus:
\begin{equation} \label{eq:sum_epsilon}
    \sum_{k=1}^{K} \varepsilon_k \le \frac{L}{2} \norm{x_0 - x^*}^2
\end{equation}

\textbf{Step 6: Obtaining the convergence rate.}
From Step 1, we know that $f(x_{k+1}) \le f(x_k)$, so the sequence $\varepsilon_k = f(x_k) - f^*$ is non-increasing ($\varepsilon_{k+1} \le \varepsilon_k$). Therefore:
\begin{equation*}
    K \cdot \varepsilon_K = K (f(x_K) - f^*) \le \sum_{k=1}^{K} \varepsilon_k
\end{equation*}
Combining this with inequality \eqref{eq:sum_epsilon}:
\begin{equation*}
    K \varepsilon_K \le \frac{L}{2} \norm{x_0 - x^*}^2
\end{equation*}
From this, we obtain the convergence rate:
\begin{equation} \label{eq:convergence_rate}
    f(x_K) - f^* = \varepsilon_K \le \frac{L \norm{x_0 - x^*}^2}{2 K}
\end{equation}

\subsection*{Conclusion}
For a convex and $L$-smooth function $f$, the standard gradient descent method with step size $\alpha = 1/L$ converges in function value to the minimum $f^*$ with a rate of $O(1/K)$. That is, $f(x_K) \to f^*$ as $K \to \infty$.

\section*{Derivation of the Preconditioned Gradient $\gradp f(x)$}

Let $f: \R^n \to \R$ be a differentiable function. Let $P$ be a symmetric positive definite matrix defining the $P$-inner product :
\begin{align*}
    \ipp{u}{v} &= u^T P v = \ip{Pu}{v} \\
    \normp{u} &= \sqrt{\ipp{u}{u}} = \sqrt{u^T P u}
\end{align*}
The differential of $f$ at $x$, denoted $df_x(h)$, is a linear functional representing the best linear approximation of the change $f(x+h) - f(x)$ for a small displacement $h$.
By the Riesz Representation Theorem, this linear functional $df_x$ can be represented via an inner product with a unique vector. The specific vector depends on the chosen inner product:

\begin{enumerate}
    \item Using the \textbf{standard inner product} $\ip{\cdot}{\cdot}$, there exists a unique vector, the \textbf{standard gradient} $\grad f(x)$, such that for all $h \in \R^n$:
        \begin{equation*}
            df_x(h) = \ip{\grad f(x)}{h}
        \end{equation*}
    \item Using the \textbf{$P$-inner product} $\ipp{\cdot}{\cdot}$, there exists a unique vector, the \textbf{preconditioned gradient} $\gradp f(x)$, such that for all $h \in \R^n$:
        \begin{equation*}
            df_x(h) = \ipp{\gradp f(x)}{h}
        \end{equation*}
\end{enumerate}

Since both expressions represent the same differential $df_x(h)$, they must be equal:
\begin{equation*}
    \ip{\grad f(x)}{h} = \ipp{\gradp f(x)}{h}
\end{equation*}
Using the definition $\ipp{u}{v} = \ip{Pu}{v}$:
\begin{equation*}
    \ip{\grad f(x)}{h} = \ip{P (\gradp f(x))}{h}
\end{equation*}
Rearranging the terms:
\begin{equation*}
    \ip{\grad f(x) - P \gradp f(x)}{h} = 0
\end{equation*}
This must hold for all $h$, which implies the vector inside the inner product must be zero:
\begin{equation*}
    \grad f(x) - P \gradp f(x) = 0
\end{equation*}
Solving for $\gradp f(x)$ (using the invertibility of $P$):
\begin{equation*}
    P \gradp f(x) = \grad f(x)
\end{equation*}
\begin{equation} \label{eq:gradp_explicit_app}
    \boxed{\gradp f(x) = P^{-1} \grad f(x)}
\end{equation}
This gives the explicit relationship between the preconditioned gradient and the standard gradient.

\vspace{\baselineskip}

\section*{Convergence of Preconditioned Gradient Descent}

\subsection*{Problem Setup}
We consider the minimization problem:
\begin{equation*}
    \min_{x \in \R^n} f(x)
\end{equation*}
where $f: \R^n \to \R$ is a convex and differentiable function.
The preconditioned gradient descent update rule is:
\begin{equation} \label{eq:pgd_update}
    x_{k+1} = x_k - \alpha \gradp f(x_k) = x_k - \alpha P^{-1} \grad f(x_k)
\end{equation}
where $\alpha > 0$ is the step size (learning rate).

\subsection*{Assumptions}
\begin{enumerate}
    \item \textbf{Convexity of $f$}: For any $x, y \in \R^n$:
          \begin{equation} \label{eq:convexity_p}
              f(y) \ge f(x) + \ip{\grad f(x)}{y-x} = f(x) + \ipp{\gradp f(x)}{y-x}
          \end{equation}
    \item \textbf{$L_P$-smoothness with respect to the $P$-norm}: We assume that the preconditioned gradient $\gradp f$ is $L_P$-Lipschitz continuous with respect to the $P$-norm. That is, there exists a constant $L_P > 0$ such that:
          \begin{equation} \label{eq:lp_smoothness}
              \normp{\gradp f(x) - \gradp f(y)} \le L_P \normp{x - y} \quad \forall x, y \in \R^n
          \end{equation}
          This assumption is equivalent to the following inequality (the $P$-Descent Lemma):
          \begin{equation} \label{eq:p_descent_lemma}
              f(y) \le f(x) + \ipp{\gradp f(x)}{y-x} + \frac{L_P}{2} \normp{y-x}^2
          \end{equation}
          
    \item \textbf{Existence of a minimizer}: There exists $x^* \in \R^n$ such that $f(x^*) = f^* = \min_{x \in \R^n} f(x)$. For convex $f$, this implies $\grad f(x^*) = 0$, and consequently $\gradp f(x^*) = P^{-1} 0 = 0$.
\end{enumerate}

\subsection*{Convergence Proof}

\textbf{Step 1: Bounding the function decrease in one step.}
We use the $P$-Descent Lemma \eqref{eq:p_descent_lemma} with $x = x_k$ and $y = x_{k+1} = x_k - \alpha \gradp f(x_k)$. Then $y - x = -\alpha \gradp f(x_k)$.
\begin{align*}
    f(x_{k+1}) &\le f(x_k) + \ipp{\gradp f(x_k)}{-\alpha \gradp f(x_k)} + \frac{L_P}{2} \normp{-\alpha \gradp f(x_k)}^2 \\
    &= f(x_k) - \alpha \ipp{\gradp f(x_k)}{\gradp f(x_k)} + \frac{L_P \alpha^2}{2} \normp{\gradp f(x_k)}^2 \\
    &= f(x_k) - \alpha \normp{\gradp f(x_k)}^2 + \frac{L_P \alpha^2}{2} \normp{\gradp f(x_k)}^2 \\
    &= f(x_k) - \alpha \left( 1 - \frac{L_P \alpha}{2} \right) \normp{\gradp f(x_k)}^2
\end{align*}
To guarantee descent, we choose the step size $\alpha$ such that $1 - \frac{L_P \alpha}{2} \ge 0$, i.e., $\alpha \le \frac{2}{L_P}$. A standard choice is $\alpha = \frac{1}{L_P}$. With this choice:
\begin{align}
    f(x_{k+1}) &\le f(x_k) - \frac{1}{L_P} \left( 1 - \frac{L_P (1/L_P)}{2} \right) \normp{\gradp f(x_k)}^2 \nonumber \\
    &= f(x_k) - \frac{1}{L_P} \left( 1 - \frac{1}{2} \right) \normp{\gradp f(x_k)}^2 \nonumber \\
    f(x_{k+1}) &\le f(x_k) - \frac{1}{2 L_P} \normp{\gradp f(x_k)}^2 \label{eq:func_decrease_p}
\end{align}
This shows that the function value decreases at each step, provided $\gradp f(x_k) \neq 0$.

\textbf{Step 2: Analyzing the distance to the optimum in $P$-norm.}
Consider the squared $P$-norm of the distance from $x_{k+1}$ to $x^*$:
\begin{align*}
    \normp{x_{k+1} - x^*}^2 &= \normp{x_k - \alpha \gradp f(x_k) - x^*}^2 \\
    &= \normp{(x_k - x^*) - \alpha \gradp f(x_k)}_P^2 \\
    &= \normp{x_k - x^*}^2 - 2 \ipp{x_k - x^*}{\alpha \gradp f(x_k)} + \normp{\alpha \gradp f(x_k)}^2 \\
    &= \normp{x_k - x^*}^2 - 2 \alpha \ipp{\gradp f(x_k)}{x_k - x^*} + \alpha^2 \normp{\gradp f(x_k)}^2
\end{align*}

\textbf{Step 3: Using convexity.}
From the convexity inequality \eqref{eq:convexity_p}, substitute $y = x^*$:
\begin{equation*}
    f(x^*) \ge f(x_k) + \ipp{\gradp f(x_k)}{x^* - x_k}
\end{equation*}
Rearranging gives:
\begin{equation} \label{eq:convex_for_dist_p}
    \ipp{\gradp f(x_k)}{x_k - x^*} \ge f(x_k) - f(x^*) = f(x_k) - f^*
\end{equation}


\textbf{Step 4: Combining the results.}
Substitute inequality \eqref{eq:convex_for_dist_p} into the expression for the distance:
\begin{equation*}
    \normp{x_{k+1} - x^*}^2 \le \normp{x_k - x^*}^2 - 2 \alpha (f(x_k) - f^*) + \alpha^2 \normp{\gradp f(x_k)}^2
\end{equation*}
Now, use the step size $\alpha = 1/L_P$ and the function decrease inequality \eqref{eq:func_decrease_p}. From \eqref{eq:func_decrease_p}, we have $\normp{\gradp f(x_k)}^2 \le 2 L_P (f(x_k) - f(x_{k+1}))$.
\begin{align*}
    \normp{x_{k+1} - x^*}^2 &\le \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_k) - f^*) + \frac{1}{L_P^2} \normp{\gradp f(x_k)}^2 \\
    &\le \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_k) - f^*) + \frac{1}{L_P^2} [2 L_P (f(x_k) - f(x_{k+1}))] \\
    &= \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_k) - f^*) + \frac{2}{L_P} (f(x_k) - f(x_{k+1})) \\
    &= \normp{x_k - x^*}^2 - \frac{2}{L_P} \left[ (f(x_k) - f^*) - (f(x_k) - f(x_{k+1})) \right] \\
    &= \normp{x_k - x^*}^2 - \frac{2}{L_P} (f(x_{k+1}) - f^*)
\end{align*}
\textbf{Step 5: Telescoping sum.}
Let $\delta_k^P = \normp{x_k - x^*}^2$ (squared $P$-distance) and $\varepsilon_k = f(x_k) - f^*$ (function error). The inequality becomes:
\begin{equation*}
    \delta_{k+1}^P \le \delta_k^P - \frac{2}{L_P} \varepsilon_{k+1}
\end{equation*}
or
\begin{equation*}
    \varepsilon_{k+1} \le \frac{L_P}{2} (\delta_k^P - \delta_{k+1}^P)
\end{equation*}
Summing this inequality from $k = 0$ to $K-1$:
\begin{align*}
    \sum_{k=0}^{K-1} \varepsilon_{k+1} &\le \sum_{k=0}^{K-1} \frac{L_P}{2} (\delta_k^P - \delta_{k+1}^P) \\
    \sum_{k=1}^{K} \varepsilon_k &\le \frac{L_P}{2} \left( \sum_{k=0}^{K-1} (\delta_k^P - \delta_{k+1}^P) \right) \\
    &= \frac{L_P}{2} (\delta_0^P - \delta_K^P) \quad \text{(Telescoping sum)}
\end{align*}
Since $\delta_K^P = \normp{x_K - x^*}^2 \ge 0$, we have $\delta_0^P - \delta_K^P \le \delta_0^P = \normp{x_0 - x^*}^2$. Thus:
\begin{equation} \label{eq:sum_epsilon_p}
    \sum_{k=1}^{K} \varepsilon_k \le \frac{L_P}{2} \normp{x_0 - x^*}^2
\end{equation}

\textbf{Step 6: Obtaining the convergence rate.}
From Step 1, we know that $f(x_{k+1}) \le f(x_k)$, so the sequence $\varepsilon_k = f(x_k) - f^*$ is non-increasing ($\varepsilon_{k+1} \le \varepsilon_k$). Therefore:
\begin{equation*}
    K \cdot \varepsilon_K = K (f(x_K) - f^*) \le \sum_{k=1}^{K} \varepsilon_k
\end{equation*}
Combining this with inequality \eqref{eq:sum_epsilon_p}:
\begin{equation*}
    K \varepsilon_K \le \frac{L_P}{2} \normp{x_0 - x^*}^2
\end{equation*}
From this, we obtain the convergence rate:
\begin{equation} \label{eq:convergence_rate_p}
    f(x_K) - f^* = \varepsilon_K \le \frac{L_P \normp{x_0 - x^*}^2}{2 K}
\end{equation}

\subsection*{Conclusion}
For a convex function $f$ that is $L_P$-smooth with respect to the $P$-norm, the preconditioned gradient descent method with step size $\alpha = 1/L_P$ converges in function value to the minimum $f^*$ with a rate of $O(1/K)$. That is, $f(x_K) \to f^*$ as $K \to \infty$.

The proof is completely analogous to the standard proof for GD, but all operations (inner product, norm, gradient, Lipschitz constant) are replaced by their $P$-analogues.

\end{document}

\end{document}