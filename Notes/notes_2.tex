\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

\newcommand{\lr}[1]{\langle #1\rangle}
\newcommand{\R}{\mathbb{R}}

\title{Gradient Descent with Weighted Inner Product}
\date{}
\begin{document}

\maketitle

\section*{Problem}

Our goal is to solve the optimization problem
\[
\min_x f(x),
\]
where \( f \colon \R^n \to \R \) is a convex and differentiable function.

Let \( P \in \R^{n \times n} \) be a symmetric positive definite matrix. This matrix induces a new (weighted) inner product defined as $\lr{x,y}_P = \lr{Px,y}$ 
which, in turn, induces a new gradient operator \( \nabla_P f(x) \) with respect to this inner product. (Why does this make sense?)

This motivates us to consider a generalized version of gradient descent using the new gradient:
\begin{equation}
\label{eq:1}
x_{k+1} = x_k - \alpha \nabla_P f(x_k),
\end{equation}
where \( \alpha > 0 \) is the step size.


\textbf{TODO:}
\begin{itemize}
\item Find an explicit form of \( \nabla_P f(x) \).
\item Think of other ingredients/assumptions you need (such as the Lipschitzness
  of \( \nabla_P f \)) and prove the convergence of \eqref{eq:1}.
\item Hint: you should understand well main ingredients of the standard
  proof of GD in the convex case and adjust them to your setting.
\end{itemize}

\section*{Base Statements}

\paragraph{Lemma 2.28.}
If $f$ is $L$–smooth and $\gamma > 0$, then for all $x, y \in \mathbb{R}^d$,
\[
f(x - \gamma \nabla f(x)) - f(x) \le -\gamma \left(1 - \frac{\gamma L}{2} \right) \|\nabla f(x)\|^2. \tag{10}
\]
If moreover $\inf f > -\infty$, then for all $x \in \mathbb{R}^d$,
\[
\frac{1}{2L} \|\nabla f(x)\|^2 \le f(x) - \inf f.
\]



\section*{General Proof of Convergence of Gradient Descent}

\textbf{Theorem} Consider the Problem (Differentiable Function) and assume that \( f \) is convex
and \( L \)-smooth, for some \( L > 0 \). Let \( (x_t)_{t\in\mathbb{N}} \) be the sequence of iterates generated by the (GD)
algorithm, with a stepsize satisfying \( 0 < \gamma \le \frac{1}{L} \). Then, for all \( x^* \in \arg\min f \), for all \( t \in \mathbb{N} \), we have:
\[
f(x_t) - \inf f \le \frac{\|x_0 - x^*\|^2}{2\gamma t}.
\]

\textbf{Proof} Let $f$ be convex and $L$–smooth. It follows that
\begin{align*}
\|x_{t+1} - x^*\|^2 
&= \left\|x_t - x^* - \frac{1}{L} \nabla f(x_t) \right\|^2 \\
&= \|x_t - x^*\|^2 
   - 2 \cdot \frac{1}{L} \langle x_t - x^*, \nabla f(x_t) \rangle 
   + \frac{1}{L^2} \|\nabla f(x_t)\|^2  \\
&\overset{(1)}{\le} \|x_t - x^*\|^2 - \frac{1}{L^2} \|\nabla f(x_t)\|^2. \tag{18}
\end{align*}

Thus, \( \|x_t - x^*\|^2 \) is a decreasing sequence in \( t \), and consequently
\begin{equation}
\|x_t - x^*\| \le \|x_0 - x^*\|. \tag{19}
\end{equation}

Calling upon (10) and subtracting \( f(x^*) \) from both sides gives
\begin{equation}
f(x_{t+1}) - f(x^*) \le f(x_t) - f(x^*) - \frac{1}{2L} \|\nabla f(x_t)\|^2. \tag{20}
\end{equation}

Applying convexity we have that
\begin{align}
f(x_t) - f(x^*) &\le \langle \nabla f(x_t), x_t - x^* \rangle \nonumber \\
&\le \|\nabla f(x_t)\| \cdot \|x_t - x^*\| \nonumber \\
&\overset{(19)}{\le} \|\nabla f(x_t)\| \cdot \|x_0 - x^*\|. \tag{21}
\end{align}

Suppose now that \( x_0 \neq x^* \), otherwise the proof is finished. Isolating \( \| \nabla f(x_t) \| \) in the above and inserting in (20) gives
\[
f(x_{t+1}) - f(x^*) \overset{(20) + (21)}{\leq} f(x_t) - f(x^*) - \frac{1}{2L} \frac{1}{\|x_0 - x^*\|^2} ( f(x_t) - f(x^*) )^2  \tag{22}
\]

Let \( \beta = \frac{1}{2L} \frac{1}{\|x_0 - x^*\|^2} \)  \quad  and \( \delta_t = f(x_t) - f(x^*) \). Since \( \delta_{t+1} \leq \delta_t \), and by manipulating (22) we have that

\[
\delta_{t+1} \leq \delta_t - \beta \delta_t^2 \xleftrightarrow{\times \frac{1}{\delta_t \delta_{t+1}}}
\beta \frac{\delta_t}{\delta_{t+1}} \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t}
\xleftrightarrow{\delta_{t+1} \leq \delta_t}
\beta \leq \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t}.
\]

Summing up both sides over $t = 0, \dots, T - 1$ and using telescopic cancellation we have that
\[
T \beta \leq \frac{1}{\delta_T} - \frac{1}{\delta_0} \leq \frac{1}{\delta_T}.
\]

Re-arranging the above we have that
\[
f(x^T) - f(x^*) = \delta_T \leq \frac{1}{\beta T} = \frac{2L \|x^0 - x^*\|^2}{T}.
\]

\section*{Proof of Convergence of Gradient Descent with weighted inner product}

%\textbf{Proof.} Consider the norm induced by \(P\): \( \|x\|_P^2 = x^\top P x \). Then,
%\begin{align*}
%\|x_{t+1} - x^*\|_P^2 
%&= \left\|x_t - x^* - \eta P^{-1} \nabla f(x_t)\right\|_P^2 \\
%&= \|x_t - x^*\|_P^2 - 2\eta \langle \nabla f(x_t), x_t - x^* \rangle + \eta^2 \|\nabla f(x_t)\|_{P^{-1}}^2,
%\end{align*}
%where \( \|\nabla f(x_t)\|_{P^{-1}}^2 = \nabla f(x_t)^\top P^{-1} \nabla f(x_t) \).

%From this, we obtain:
%\[
%\|x_{t+1} - x^*\|_P^2 \le \|x_t - x^*\|_P^2 - 2\eta \langle \nabla f(x_t), x_t - x^* \rangle + \eta^2 \|\nabla f(x_t)\|_{P^{-1}}^2.
%\]

%Since \(f\) is convex and \(L_P\)-smooth in the \(P\)-norm, we can use the inequality:
%\[
%f(x_{t+1}) \le f(x_t) - \eta \|\nabla f(x_t)\|_{P^{-1}}^2 + \frac{L_P \eta^2}{2} \|\nabla f(x_t)\|_{P^{-1}}^2
%= f(x_t) - \left(\eta - \frac{L_P \eta^2}{2} \right)\|\nabla f(x_t)\|_{P^{-1}}^2.
%\]

%Choosing \( \eta = \frac{1}{L_P} \), we simplify the inequality:
%\[
%f(x_{t+1}) \le f(x_t) - \frac{1}{2 L_P} \|\nabla f(x_t)\|_{P^{-1}}^2.
%\]

%Now using convexity again:
%\[
%f(x_t) - f(x^*) \le \langle \nabla f(x_t), x_t - x^* \rangle \le \|\nabla f(x_t)\|_{P^{-1}} \cdot \|x_t - x^*\|_P \le \|\nabla f(x_t)\|_{P^{-1}} \cdot \|x_0 - x^*\|_P.
%\]

%Solving for \( \|\nabla f(x_t)\|_{P^{-1}} \) and substituting gives:
%\[
%f(x_{t+1}) - f(x^*) \le f(x_t) - f(x^*) - \frac{1}{2 L_P} \cdot \frac{(f(x_t) - f(x^*))^2}{\|x_0 - x^*\|_P^2}.
%\]

%Letting \( \delta_t := f(x_t) - f(x^*) \), and \( \beta = \frac{1}{2 L_P \|x_0 - x^*\|_P^2} \), we get:
%\[
%\delta_{t+1} \le \delta_t - \beta \delta_t^2.
%\]

%Then, as before:
%\[
%\beta \le \frac{1}{\delta_{t+1}} - \frac{1}{\delta_t} \quad \Rightarrow \quad T \beta \le \frac{1}{\delta_T} - \frac{1}{\delta_0} \le \frac{1}{\delta_T},
%\]
%hence:
%\[
%f(x_T) - f(x^*) = \delta_T \le \frac{1}{\beta T} = \frac{2 L_P \|x^0 - x^*\|_P^2}{T}.
%\]
%\qed

\section*{Main Results}

We consider gradient descent in a space equipped with a weighted inner product:
\[
\langle x, y \rangle_P := \langle P x, y \rangle = x^\top P y,
\]
where \(P\) is a symmetric positive definite matrix.

In this geometry, the gradient descent update takes the form:
\[
x_{t+1} = x_t - \eta P^{-1} \nabla f(x_t),
\]
where \(\eta > 0\) is the learning rate and \(\nabla f(x_t)\) is the usual Euclidean gradient.

This is equivalent to performing preconditioned gradient descent with preconditioner \(P^{-1}\), which adapts the step direction to the local geometry defined by \(P\).


\end{document}
 